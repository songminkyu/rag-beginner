#!/usr/bin/env python3
"""
EXAONE 4.0 ëª¨ë¸ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
Hugging Face Transformersë¥¼ í†µí•œ EXAONE 4.0 ëª¨ë¸ ì„¤ì •
"""

import os
import sys
import subprocess
import logging
import torch
from pathlib import Path
from typing import Optional, Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ìƒ‰ìƒ ì¶œë ¥ì„ ìœ„í•œ ANSI ì½”ë“œ
class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    PURPLE = '\033[0;35m'
    CYAN = '\033[0;36m'
    WHITE = '\033[1;37m'
    NC = '\033[0m'  # No Color

def print_colored(message: str, color: str = Colors.WHITE):
    """ìƒ‰ìƒ ì¶œë ¥"""
    print(f"{color}{message}{Colors.NC}")

def print_info(message: str):
    print_colored(f"â„¹ï¸  {message}", Colors.BLUE)

def print_success(message: str):
    print_colored(f"âœ… {message}", Colors.GREEN)

def print_warning(message: str):
    print_colored(f"âš ï¸  {message}", Colors.YELLOW)

def print_error(message: str):
    print_colored(f"âŒ {message}", Colors.RED)

def check_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    print_info("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
    
    # Python ë²„ì „ í™•ì¸
    python_version = sys.version_info
    if python_version < (3, 9):
        print_error(f"Python 3.9 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {python_version.major}.{python_version.minor}")
        return False
    else:
        print_success(f"Python ë²„ì „: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    # PyTorch ì„¤ì¹˜ í™•ì¸
    try:
        import torch
        print_success(f"PyTorch ë²„ì „: {torch.__version__}")
        
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print_success(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {gpu_count}ê°œ GPU, {gpu_name}, {memory:.1f}GB")
        else:
            print_warning("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            
    except ImportError:
        print_error("PyTorchê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    # ë©”ëª¨ë¦¬ í™•ì¸
    try:
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory < 16:
                print_warning(f"GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {gpu_memory:.1f}GB")
                print_info("EXAONE-4.0-32B ëª¨ë¸ì€ ìµœì†Œ 24GB GPU ë©”ëª¨ë¦¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                print_success(f"ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        
        # RAM í™•ì¸ (Linux/macOS)
        if sys.platform != "win32":
            import psutil
            ram_gb = psutil.virtual_memory().total / 1024**3
            print_info(f"ì‹œìŠ¤í…œ RAM: {ram_gb:.1f}GB")
            
    except Exception as e:
        print_warning(f"ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    return True

def install_dependencies():
    """í•„ìš”í•œ ì˜ì¡´ì„± ì„¤ì¹˜"""
    print_info("EXAONE ì§€ì›ì„ ìœ„í•œ ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘...")
    
    # í•„ìˆ˜ íŒ¨í‚¤ì§€ ëª©ë¡
    required_packages = [
        "torch>=2.3.0",
        "accelerate>=0.32.0",
        "sentence-transformers>=3.0.0",
        "bitsandbytes",  # ë©”ëª¨ë¦¬ ìµœì í™”
        "flash-attn",    # ì£¼ì˜: ì„¤ì¹˜ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
    ]
    
    # EXAONE ì§€ì› transformers ì„¤ì¹˜
    print_info("EXAONE ì§€ì› transformers ì„¤ì¹˜ ì¤‘...")
    try:
        subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "git+https://github.com/lgai-exaone/transformers@add-exaone4"
        ], check=True, capture_output=True)
        print_success("EXAONE ì§€ì› transformers ì„¤ì¹˜ ì™„ë£Œ")
    except subprocess.CalledProcessError as e:
        print_warning("EXAONE ì§€ì› transformers ì„¤ì¹˜ ì‹¤íŒ¨, ê¸°ë³¸ transformers ì‚¬ìš©")
        subprocess.run([sys.executable, "-m", "pip", "install", "transformers>=4.45.0"], check=True)
    
    # ê¸°íƒ€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
    for package in required_packages:
        try:
            print_info(f"ì„¤ì¹˜ ì¤‘: {package}")
            subprocess.run([sys.executable, "-m", "pip", "install", package], 
                         check=True, capture_output=True)
            print_success(f"âœ“ {package}")
        except subprocess.CalledProcessError:
            if "flash-attn" in package:
                print_warning(f"âœ— {package} (ì„ íƒì‚¬í•­, ì„¤ì¹˜ ì‹¤íŒ¨)")
            else:
                print_error(f"âœ— {package} ì„¤ì¹˜ ì‹¤íŒ¨")

def get_model_recommendations():
    """ì‹œìŠ¤í…œì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
    recommendations = {}
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        if gpu_memory >= 32:
            recommendations["recommended"] = "LGAI-EXAONE/EXAONE-4.0-32B"
            recommendations["reason"] = "ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥"
        elif gpu_memory >= 8:
            recommendations["recommended"] = "LGAI-EXAONE/EXAONE-4.0-1.2B"
            recommendations["reason"] = "GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” ì†Œí˜• ëª¨ë¸ ê¶Œì¥"
        else:
            recommendations["recommended"] = "LGAI-EXAONE/EXAONE-4.0-1.2B"
            recommendations["reason"] = "ì œí•œëœ GPU ë©”ëª¨ë¦¬ë¡œ ì†Œí˜• ëª¨ë¸ í•„ìˆ˜"
    else:
        recommendations["recommended"] = "LGAI-EXAONE/EXAONE-4.0-1.2B"
        recommendations["reason"] = "CPU í™˜ê²½ì—ì„œëŠ” ì†Œí˜• ëª¨ë¸ ê¶Œì¥"
    
    return recommendations

def test_model_loading(model_name: str) -> bool:
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print_info(f"ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸: {model_name}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸
        print_info("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print_success("í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
        
        # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„°ë§Œ)
        print_info("ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸ ì¤‘...")
        model_config = AutoModelForCausalLM.from_pretrained(
            model_name, 
            torch_dtype=torch.bfloat16,
            device_map="cpu",  # ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print_success("ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸ ì„±ê³µ")
        
        # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
        test_text = "ì•ˆë…•í•˜ì„¸ìš”, EXAONE ëª¨ë¸ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
        tokens = tokenizer.encode(test_text)
        decoded = tokenizer.decode(tokens)
        
        print_success(f"í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(tokens)} í† í°")
        
        return True
        
    except Exception as e:
        print_error(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def create_test_script():
    """í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    test_script_content = '''#!/usr/bin/env python3
"""
EXAONE 4.0 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_exaone_model(model_name="LGAI-EXAONE/EXAONE-4.0-1.2B"):
    """EXAONE ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    print(f"ğŸš€ EXAONE ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_name}")
    print("=" * 50)
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info("ëª¨ë¸ ë¡œë”© ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
        print(f"ğŸ“ ë””ë°”ì´ìŠ¤: {model.device}")
        print(f"ğŸ§® dtype: {model.dtype}")
        
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} GB")
        
        # í…ŒìŠ¤íŠ¸ ìƒì„±
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
            "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ ì¤‘ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”."
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\\nğŸ”„ í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            print("-" * 30)
            
            messages = [{"role": "user", "content": prompt}]
            input_ids = tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            with torch.no_grad():
                output = model.generate(
                    input_ids.to(model.device),
                    max_new_tokens=128,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_tokens = output[0][len(input_ids[0]):]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            print(f"ğŸ’¬ ì‘ë‹µ: {response.strip()}")
        
        print("\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    return True

if __name__ == "__main__":
    import sys
    
    model_name = sys.argv[1] if len(sys.argv) > 1 else "LGAI-EXAONE/EXAONE-4.0-1.2B"
    test_exaone_model(model_name)
'''
    
    # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ìƒì„±
    test_script_path = Path("scripts/test_exaone.py")
    test_script_path.parent.mkdir(exist_ok=True)
    
    with open(test_script_path, "w", encoding="utf-8") as f:
        f.write(test_script_content)
    
    # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬ (Unix ê³„ì—´)
    if sys.platform != "win32":
        import stat
        st = os.stat(test_script_path)
        os.chmod(test_script_path, st.st_mode | stat.S_IEXEC)
    
    print_success(f"í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: {test_script_path}")
    return test_script_path

def update_env_file():
    """í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ì—…ë°ì´íŠ¸"""
    print_info("í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì—…ë°ì´íŠ¸ ì¤‘...")
    
    env_file = Path(".env")
    env_example = Path(".env.example")
    
    # .env íŒŒì¼ì´ ì—†ìœ¼ë©´ .env.exampleì—ì„œ ë³µì‚¬
    if not env_file.exists() and env_example.exists():
        import shutil
        shutil.copy(env_example, env_file)
        print_success(".env íŒŒì¼ ìƒì„± ì™„ë£Œ")
    
    # ê¶Œì¥ ëª¨ë¸ ì„¤ì •
    recommendations = get_model_recommendations()
    recommended_model = recommendations["recommended"]
    
    print_info(f"ê¶Œì¥ ëª¨ë¸: {recommended_model}")
    print_info(f"ì´ìœ : {recommendations['reason']}")
    
    # .env íŒŒì¼ ì—…ë°ì´íŠ¸
    if env_file.exists():
        with open(env_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        # EXAONE_MODEL ì„¤ì • ì—…ë°ì´íŠ¸
        if "EXAONE_MODEL=" in content:
            # ê¸°ì¡´ ì„¤ì • ì£¼ì„ ì²˜ë¦¬í•˜ê³  ìƒˆë¡œìš´ ì„¤ì • ì¶”ê°€
            lines = content.split("\n")
            new_lines = []
            
            for line in lines:
                if line.startswith("EXAONE_MODEL="):
                    new_lines.append(f"# {line}")
                    new_lines.append(f"EXAONE_MODEL={recommended_model}")
                else:
                    new_lines.append(line)
            
            content = "\n".join(new_lines)
        else:
            # ìƒˆë¡œìš´ ì„¤ì • ì¶”ê°€
            content += f"\n# EXAONE 4.0 Model Configuration\nEXAONE_MODEL={recommended_model}\n"
        
        with open(env_file, "w", encoding="utf-8") as f:
            f.write(content)
        
        print_success("í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print_colored("ğŸš€ EXAONE 4.0 ëª¨ë¸ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸", Colors.CYAN)
    print_colored("=" * 50, Colors.CYAN)
    print()
    
    # 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not check_system_requirements():
        print_error("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    print()
    
    # 2. ì˜ì¡´ì„± ì„¤ì¹˜
    try:
        install_dependencies()
        print_success("ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ")
    except Exception as e:
        print_error(f"ì˜ì¡´ì„± ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
        return False
    
    print()
    
    # 3. ëª¨ë¸ ì¶”ì²œ ë° í…ŒìŠ¤íŠ¸
    recommendations = get_model_recommendations()
    recommended_model = recommendations["recommended"]
    
    print_info(f"ê¶Œì¥ ëª¨ë¸: {recommended_model}")
    print_info(f"ì´ìœ : {recommendations['reason']}")
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1) LGAI-EXAONE/EXAONE-4.0-1.2B (ì†Œí˜•, ë¹ ë¦„)")
    print("2) LGAI-EXAONE/EXAONE-4.0-32B (ëŒ€í˜•, ê³ ì„±ëŠ¥)")
    print(f"3) ê¶Œì¥ ëª¨ë¸ ì‚¬ìš© ({recommended_model})")
    
    choice = input("\nì„ íƒ (1-3): ").strip()
    
    model_map = {
        "1": "LGAI-EXAONE/EXAONE-4.0-1.2B",
        "2": "LGAI-EXAONE/EXAONE-4.0-32B", 
        "3": recommended_model
    }
    
    selected_model = model_map.get(choice, recommended_model)
    
    print_info(f"ì„ íƒëœ ëª¨ë¸: {selected_model}")
    
    # 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸
    if test_model_loading(selected_model):
        print_success("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print_warning("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    print()
    
    # 5. í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸
    update_env_file()
    
    # 6. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    test_script_path = create_test_script()
    
    # 7. ì‚¬ìš©ë²• ì•ˆë‚´
    print()
    print_colored("ğŸ‰ EXAONE 4.0 ì„¤ì • ì™„ë£Œ!", Colors.GREEN)
    print("=" * 50)
    print()
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. ì „ì²´ í…ŒìŠ¤íŠ¸: python {test_script_path}")
    print("2. Hello RAG ì‹¤í–‰: python tutorials/01_getting_started/hello_rag.py")
    print("3. ëª¨ë¸ ë³€ê²½: .env íŒŒì¼ì—ì„œ EXAONE_MODEL ìˆ˜ì •")
    print()
    print("ë¬¸ì œ í•´ê²°:")
    print("- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ë˜ëŠ” batch_size ê°ì†Œ")
    print("- ì†ë„ ê°œì„ : GPU ì‚¬ìš©, flash-attention ì„¤ì¹˜")
    print("- í•œêµ­ì–´ ì„±ëŠ¥: EXAONE ëª¨ë¸ì´ í•œêµ­ì–´ì— ìµœì í™”ë˜ì–´ ìˆìŒ")
    print()
    print_success("ì¦ê±°ìš´ RAG í•™ìŠµ ë˜ì„¸ìš”! ğŸš€")
    
    return True

if __name__ == "__main__":
    main()