"""
Hello RAG - ì²« ë²ˆì§¸ RAG ì‹œìŠ¤í…œ êµ¬í˜„
ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì—¬ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import math

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenvë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install python-dotenv")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleRAG:
    """ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„"""
    
    def __init__(
        self,
        llm_provider_type: str = "openai",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """SimpleRAG ì´ˆê¸°í™”"""
        
        self.llm_provider_type = llm_provider_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.documents = []  # ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ì €ì¥
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _initialize_components(self):
        """RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        
        # LLM ì œê³µì ì´ˆê¸°í™”
        self.llm_provider = self._create_llm_provider()
        
        logger.info("RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_llm_provider(self):
        """LLM ì œê³µì ìƒì„±"""
        
        if self.llm_provider_type == "openai":
            try:
                from src.core.llm_providers.openai_provider import OpenAIProvider
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                config = {
                    "api_key": api_key,
                    "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
                return OpenAIProvider(config)
            except ImportError:
                logger.error("OpenAI ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise
        
        elif self.llm_provider_type == "claude":
            try:
                from src.core.llm_providers.claude_provider import ClaudeProvider
                api_key = os.getenv("ANTHROPIC_API_KEY")
                if not api_key:
                    raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                config = {
                    "api_key": api_key,
                    "model": os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022"),
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
                return ClaudeProvider(config)
            except ImportError:
                logger.error("Claude ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise
        
        elif self.llm_provider_type == "local":
            try:
                from src.core.llm_providers.local_provider import LocalProvider
                config = {
                    "model": os.getenv("EXAONE_MODEL_NAME", "LGAI-EXAONE/EXAONE-4.0-7.8B"),
                    "device": "auto",
                    "torch_dtype": "bfloat16",
                    "korean_optimized": True
                }
                return LocalProvider(config)
            except ImportError:
                logger.error("ë¡œì»¬ ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.llm_provider_type}")
    
    def add_documents(self, documents: List[str]):
        """ë¬¸ì„œë“¤ì„ ì¶”ê°€í•˜ê³  ì„ë² ë”© ìƒì„±"""
        logger.info(f"Adding {len(documents)} documents...")
        
        self.documents.extend(documents)
        
        # ì„ë² ë”© ìƒì„±
        new_embeddings = self.embedding_provider.embed_texts(documents)
        self.embeddings.extend(new_embeddings)
        
        logger.info(f"Total documents: {len(self.documents)}")
    
    def similarity_search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°"""
        if not self.documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_provider.embed_text(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            similarity = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append({
                'index': i,
                'document': self.documents[i],
                'similarity': similarity
            })
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:top_k]
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import math
        
        dot_product = sum(x * y for x, y in zip(a, b))
        magnitude_a = math.sqrt(sum(x * x for x in a))
        magnitude_b = math.sqrt(sum(x * x for x in b))
        
        if magnitude_a == 0 or magnitude_b == 0:
            return 0
        
        return dot_product / (magnitude_a * magnitude_b)
    
    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì‹¤í–‰"""
        logger.info(f"Processing query: {question}")
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.similarity_search(question, top_k)
        
        if not relevant_docs:
            return {
                'question': question,
                'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'sources': [],
                'context': ''
            }
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc['document'] for doc in relevant_docs])
        
        # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        rag_prompt = self.llm.format_rag_prompt(question, context)
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        response = self.llm.generate(rag_prompt)
        
        return {
            'question': question,
            'answer': response.content,
            'sources': [doc['document'][:100] + '...' for doc in relevant_docs],
            'context': context,
            'similarities': [doc['similarity'] for doc in relevant_docs]
        }


def demo_korean_rag():
    """í•œêµ­ì–´ RAG ë°ëª¨"""
    print("ğŸš€ í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ë°ëª¨")
    print("=" * 50)
    
    # í•œêµ­ì–´ ë¬¸ì„œ ìƒ˜í”Œ
    korean_documents = [
        "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥, ìì—°ì–¸ì–´ì˜ ì´í•´ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì‹¤í˜„í•œ ê¸°ìˆ ì´ë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ë²•ì„ ê°œë°œí•˜ëŠ” ë¶„ì•¼ì´ë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì´ë‹¤.",
        "ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ë‹¤.",
        "ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ë””ì§€í„¸ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì´ë‹¤.",
        "ë¡œë´‡ê³µí•™ì€ ë¡œë´‡ì˜ ì„¤ê³„, ì œì‘, ì‘ë™ì— ê´€í•œ ê¸°ìˆ ì„ ë‹¤ë£¨ëŠ” ê³µí•™ ë¶„ì•¼ì´ë‹¤.",
        "ë¹…ë°ì´í„°ëŠ” ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ë„êµ¬ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì €ì¥, ê´€ë¦¬, ë¶„ì„í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ë„˜ì–´ì„œëŠ” ëŒ€ëŸ‰ì˜ ì •í˜• ë˜ëŠ” ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë§í•œë‹¤."
    ]
    
    # SimpleRAG ì´ˆê¸°í™” (ë¡œì»¬ EXAONE ëª¨ë¸ ì‚¬ìš©)
    try:
        rag = SimpleRAG("local")
    except Exception as e:
        print(f"ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("OpenAI APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        rag = SimpleRAG("openai")
    
    # ë¬¸ì„œ ì¶”ê°€
    rag.add_documents(korean_documents)
    
    # ì§ˆë¬¸ë“¤
    questions = [
        "ì¸ê³µì§€ëŠ¥ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?",
        "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ë¹…ë°ì´í„°ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹¤í–‰
    for question in questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("-" * 30)
        
        result = rag.query(question)
        
        print(f"ğŸ’¡ ë‹µë³€: {result['answer']}")
        print(f"ğŸ“Š ìœ ì‚¬ë„: {[f'{sim:.3f}' for sim in result['similarities']]}")
        print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['sources'])}ê°œ")


def demo_english_rag():
    """ì˜ì–´ RAG ë°ëª¨"""
    print("\n\nğŸš€ English RAG System Demo")
    print("=" * 50)
    
    # ì˜ì–´ ë¬¸ì„œ ìƒ˜í”Œ
    english_documents = [
        "Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.",
        "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.",
        "Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world.",
        "Reinforcement learning is an area of machine learning where an agent learns to behave in an environment by performing actions and receiving rewards or penalties.",
        "Big data refers to extremely large datasets that may be analyzed computationally to reveal patterns, trends, and associations."
    ]
    
    # SimpleRAG ì´ˆê¸°í™”
    try:
        rag = SimpleRAG("local")
    except:
        rag = SimpleRAG("openai")
    
    # ë¬¸ì„œ ì¶”ê°€
    rag.add_documents(english_documents)
    
    # ì˜ì–´ ì§ˆë¬¸ë“¤
    questions = [
        "What is machine learning?",
        "How does deep learning differ from traditional machine learning?",
        "What is natural language processing used for?",
        "Explain computer vision"
    ]
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹¤í–‰
    for question in questions:
        print(f"\nâ“ Question: {question}")
        print("-" * 30)
        
        result = rag.query(question)
        
        print(f"ğŸ’¡ Answer: {result['answer']}")
        print(f"ğŸ“Š Similarities: {[f'{sim:.3f}' for sim in result['similarities']]}")
        print(f"ğŸ“š Sources: {len(result['sources'])} documents")


def interactive_rag():
    """ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ"""
    print("\n\nğŸ’¬ ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ê¸°ë³¸ ë¬¸ì„œ ë¡œë“œ
    documents = [
        "Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "DjangoëŠ” Pythonìœ¼ë¡œ ì‘ì„±ëœ ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ, ë¹ ë¥¸ ê°œë°œê³¼ ê¹”ë”í•œ ë””ìì¸ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "FastAPIëŠ” í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ ì›¹ APIë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        "NumPyëŠ” Pythonì—ì„œ ê³¼í•™ ê³„ì‚°ì„ ìœ„í•œ ê¸°ë³¸ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤.",
        "PandasëŠ” ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
        "TensorFlowëŠ” Googleì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        "PyTorchëŠ” Facebookì—ì„œ ê°œë°œí•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
    ]
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        rag = SimpleRAG("local")
        print("âœ… ë¡œì»¬ EXAONE ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    except:
        try:
            rag = SimpleRAG("openai")
            print("âœ… OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except:
            rag = SimpleRAG("claude")
            print("âœ… Claude ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    rag.add_documents(documents)
    print(f"ğŸ“š {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    while True:
        try:
            question = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë']:
                print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not question:
                continue
            
            print("ğŸ” ê²€ìƒ‰ ì¤‘...")
            result = rag.query(question)
            
            print(f"\nğŸ’¡ ë‹µë³€: {result['answer']}")
            print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(result['sources'])}")
            
            # ìƒì„¸ ì •ë³´ í‘œì‹œ ì—¬ë¶€ ë¬»ê¸°
            show_details = input("\nìƒì„¸ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if show_details in ['y', 'yes', 'ã…‡']:
                print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                for i, source in enumerate(result['sources'], 1):
                    print(f"  {i}. {source}")
                print(f"\nğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {[f'{sim:.3f}' for sim in result['similarities']]}")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ Hello RAG - ì²« ë²ˆì§¸ RAG ì˜ˆì œ")
    print("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
    available_providers = config_manager.get_available_providers()
    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì: {available_providers}")
    
    if not available_providers:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ Ollamaë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        # ë°ëª¨ ì‹¤í–‰
        demo_korean_rag()
        demo_english_rag()
        
        # ëŒ€í™”í˜• RAG
        interactive_rag()
        
    except Exception as e:
        logger.error(f"Demo ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("\nâŒ ë°ëª¨ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()