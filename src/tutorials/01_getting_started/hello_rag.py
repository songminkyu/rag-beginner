"""
Hello RAG - ì²« ë²ˆì§¸ RAG ì‹œìŠ¤í…œ êµ¬í˜„
ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì—¬ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import math
from src.config.api_config import config_manager
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenvë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install python-dotenv")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SimpleRAG:
    """ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„"""
    
    def __init__(
        self,
        llm_provider_type: str = "openai",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """SimpleRAG ì´ˆê¸°í™”"""
        
        self.llm_provider_type = llm_provider_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.documents = []  # ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ì €ì¥
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _initialize_components(self):
        """RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        
        # LLM ì œê³µì ì´ˆê¸°í™”
        self.llm_provider = self._create_llm_provider()
        
        logger.info("RAG ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_llm_provider(self):
        """LLM ì œê³µì ìƒì„±"""
        
        if self.llm_provider_type == "openai":
            try:
                from src.core.llm_providers.openai_provider import OpenAIProvider, OpenAIEmbeddingProvider
                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                config = {
                    "api_key": api_key,
                    "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                    "temperature": 0.1,
                    "max_tokens": 1000,
                    "embedding_model": os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
                }
                
                # ì„ë² ë”© ì œê³µìë„ ìƒì„±
                self.embedding_provider = OpenAIEmbeddingProvider(config)
                
                return OpenAIProvider(config)
            except ImportError:
                logger.error("OpenAI ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise
        
        elif self.llm_provider_type == "claude":
            try:
                from src.core.llm_providers.claude_provider import ClaudeProvider
                
                api_key = os.getenv("ANTHROPIC_API_KEY")
                if not api_key:
                    raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
                config = {
                    "api_key": api_key,
                    "model": os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022"),
                    "temperature": 0.1,
                    "max_tokens": 1000
                }
                
                # ClaudeëŠ” ì„ë² ë”©ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ fallback ì„ë² ë”© ì‚¬ìš©
                try:
                    from src.core.llm_providers.local_provider import LocalEmbeddingProvider
                    embedding_config = {
                        "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "korean_optimized": True
                    }
                    self.embedding_provider = LocalEmbeddingProvider(embedding_config)
                except Exception as emb_error:
                    logger.warning(f"ë¡œì»¬ ì„ë² ë”© ì œê³µì ë¡œë“œ ì‹¤íŒ¨: {emb_error}")
                    logger.info("ê¸°ë³¸ ì„ë² ë”© ì œê³µìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
                    
                    # ê°„ë‹¨í•œ ì„ë² ë”© ì œê³µì fallback
                    class SimpleEmbeddingProvider:
                        def embed_text(self, text):
                            return [0.0] * 384  # ê¸°ë³¸ ì°¨ì›
                        
                        def embed_texts(self, texts):
                            return [[0.0] * 384 for _ in texts]
                    
                    self.embedding_provider = SimpleEmbeddingProvider()
                
                return ClaudeProvider(config)
            except ImportError:
                logger.error("Claude ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise
        
        elif self.llm_provider_type == "local":
            try:
                # Try to import local providers with dependency check
                import torch
                logger.info("PyTorch found, attempting to load local providers...")
                import pyarrow
                
                # Check for sentence-transformers compatibility
                try:
                    from src.core.llm_providers.local_provider import LocalProvider, LocalEmbeddingProvider
                    
                    config = {
                        "model": os.getenv("EXAONE_MODEL_NAME", "LGAI-EXAONE/EXAONE-4.0-1.2B"),
                        "device": "auto",  
                        "torch_dtype": "bfloat16",
                        "korean_optimized": True
                    }
                    
                    # ë¡œì»¬ ì„ë² ë”© ì œê³µì ìƒì„±
                    embedding_config = {
                        "embedding_model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "korean_optimized": True
                    }
                    self.embedding_provider = LocalEmbeddingProvider(embedding_config)
                    
                    return LocalProvider(config)
                    
                except Exception as st_error:
                    logger.warning(f"SentenceTransformers í˜¸í™˜ì„± ë¬¸ì œ: {st_error}")
                    logger.info("ê¸°ë³¸ ë¡œì»¬ ì œê³µìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì„ë² ë”© ì—†ìŒ)")
                    
                    # ì™„ì „íˆ ë…ë¦½ì ì¸ fallback í´ë˜ìŠ¤ë“¤ - ì„í¬íŠ¸ ì—†ì´ êµ¬í˜„
                    from dataclasses import dataclass
                    from typing import Dict, Any, Optional
                    
                    @dataclass
                    class LLMResponse:
                        content: str
                        model: str
                        metadata: Dict[str, Any]
                    
                    class SimpleLocalProvider:
                        def __init__(self, config):
                            self.config = config
                            self.temperature = config.get("temperature", 0.1)
                            self.max_tokens = config.get("max_tokens", 1000)
                            
                        def generate(self, prompt, system_prompt=None, **kwargs):
                            return LLMResponse(
                                content="ë¡œì»¬ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAI ë˜ëŠ” Claude APIë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
                                model="local_fallback",
                                metadata={"provider": "fallback"}
                            )
                    
                    # ê°„ë‹¨í•œ ì„ë² ë”© ì œê³µì fallback
                    class SimpleEmbeddingProvider:
                        def embed_text(self, text):
                            return [0.0] * 384  # ê¸°ë³¸ ì°¨ì›
                        
                        def embed_texts(self, texts):
                            return [[0.0] * 384 for _ in texts]
                    
                    config = {"korean_optimized": True}
                    self.embedding_provider = SimpleEmbeddingProvider()
                    
                    return SimpleLocalProvider(config)
                    
            except ImportError as torch_error:
                logger.error(f"PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {torch_error}")
                logger.error("ë¡œì»¬ ì œê³µìë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ PyTorchë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install torch")
                raise ValueError("ë¡œì»¬ ì œê³µì ì‚¬ìš©ì„ ìœ„í•´ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"ë¡œì»¬ ì œê³µì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise ValueError(f"ë¡œì»¬ ì œê³µìë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.llm_provider_type}")
    
    def add_documents(self, documents: List[str], metadata_list: Optional[List[Dict]] = None):
        """ë¬¸ì„œë“¤ì„ ì‹œìŠ¤í…œì— ì¶”ê°€"""
        
        logger.info(f"{len(documents)}ê°œ ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘...")
        
        for i, doc in enumerate(documents):
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = self._simple_text_splitter(doc)
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            doc_metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}
            
            for j, chunk in enumerate(chunks):
                chunk_data = {
                    "content": chunk,
                    "metadata": {
                        "document_id": i,
                        "chunk_id": j,
                        "source": doc_metadata.get("source", f"document_{i}"),
                        **doc_metadata
                    }
                }
                self.documents.append(chunk_data)
        
        logger.info(f"ì´ {len(self.documents)}ê°œ ì²­í¬ê°€ ì¶”ê°€ë¨")
    
    def _simple_text_splitter(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„í• ê¸°"""
        
        # ê°„ë‹¨í•œ ë¶„í• : í…ìŠ¤íŠ¸ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(text) <= self.chunk_size:
            return [text]
        
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = []
        start = 0
        
        while start < len(text):
            # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
            end = start + self.chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë¼ë©´ ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
            if end < len(text):
                # ê³µë°±ì´ë‚˜ ë¬¸ì¥ ë¶€í˜¸ì—ì„œ ë¶„í• í•˜ë ¤ê³  ì‹œë„
                for i in range(end, start + self.chunk_size - self.chunk_overlap, -1):
                    if text[i] in ' \n\t.!?':
                        end = i + 1
                        break
            
            # ì²­í¬ ì¶”ì¶œ
            chunk = text[start:end].strip()
            if chunk:  # ë¹ˆ ì²­í¬ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì‹œì‘ì  ê³„ì‚° (ì˜¤ë²„ë© ì ìš©)
            start = end - self.chunk_overlap
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start >= len(text):
                break
        
        return chunks
    
    def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰"""
        
        if not self.documents:
            return []
        
        # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì„ë² ë”© ê¸°ë°˜)
        scored_docs = []
        for doc in self.documents:
            score = self._simple_similarity(query, doc["content"])
            scored_docs.append({
                "content": doc["content"],
                "metadata": doc["metadata"],
                "similarity_score": score
            })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_docs.sort(key=lambda x: x["similarity_score"], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        top_docs = scored_docs[:k]
        
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(top_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        return top_docs
    
    def _simple_similarity(self, query: str, text: str) -> float:
        """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (í‚¤ì›Œë“œ ê¸°ë°˜)"""
        
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())
        
        if not query_words or not text_words:
            return 0.0
        
        intersection = query_words.intersection(text_words)
        union = query_words.union(text_words)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import math
        
        dot_product = sum(x * y for x, y in zip(a, b))
        magnitude_a = math.sqrt(sum(x * x for x in a))
        magnitude_b = math.sqrt(sum(x * x for x in b))
        
        if magnitude_a == 0 or magnitude_b == 0:
            return 0
        
        return dot_product / (magnitude_a * magnitude_b)
    
    def generate_answer(self, query: str, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        if hasattr(self.llm_provider, 'format_rag_prompt'):
            prompt = self.llm_provider.format_rag_prompt(query=query, context=context)
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
                    ì»¨í…ìŠ¤íŠ¸:
                    {context}
                    
                    ì§ˆë¬¸: {query}
                    
                    ë‹µë³€:"""
        
        # ë‹µë³€ ìƒì„±
        response = self.llm_provider.generate(prompt)
        return response.content
    
    def query(self, question: str, k: int = 5) -> Dict[str, Any]:
        """RAG ì§ˆì˜ì‘ë‹µ ì‹¤í–‰"""
        
        logger.info(f"ì§ˆë¬¸: {question}")
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.retrieve(question, k=k)
        
        if not retrieved_docs:
            return {
                "question": question,
                "answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "context": ""
            }
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        sources = []
        
        for doc in retrieved_docs:
            if doc["similarity_score"] > 0:  # ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ
                context_parts.append(doc["content"])
                
                # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                source_info = {
                    "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                    "metadata": doc["metadata"],
                    "similarity_score": doc["similarity_score"]
                }
                sources.append(source_info)
        
        if not context_parts:
            return {
                "question": question,
                "answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "context": ""
            }
        
        context = "\n\n".join(context_parts)
        
        # 3. ë‹µë³€ ìƒì„±
        try:
            answer = self.generate_answer(question, context)
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
        result = {
            "question": question,
            "answer": answer,
            "sources": sources,
            "context": context,
            "retrieved_count": len(sources)
        }
        
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ")
        return result


def demo_korean_rag():
    """í•œêµ­ì–´ RAG ë°ëª¨"""
    print("ğŸš€ í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ë°ëª¨")
    print("=" * 50)
    
    # í•œêµ­ì–´ ë¬¸ì„œ ìƒ˜í”Œ
    korean_documents = [
        "ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥, ìì—°ì–¸ì–´ì˜ ì´í•´ëŠ¥ë ¥ ë“±ì„ ì»´í“¨í„° í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì‹¤í˜„í•œ ê¸°ìˆ ì´ë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê¸°ë²•ì„ ê°œë°œí•˜ëŠ” ë¶„ì•¼ì´ë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ë¡œ, ì¸ê³µì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì»´í“¨í„°ê°€ ì‚¬ëŒì²˜ëŸ¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì´ë‹¤.",
        "ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ë‹¤.",
        "ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ë””ì§€í„¸ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¥¼ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì´ë‹¤.",
        "ë¡œë´‡ê³µí•™ì€ ë¡œë´‡ì˜ ì„¤ê³„, ì œì‘, ì‘ë™ì— ê´€í•œ ê¸°ìˆ ì„ ë‹¤ë£¨ëŠ” ê³µí•™ ë¶„ì•¼ì´ë‹¤.",
        "ë¹…ë°ì´í„°ëŠ” ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ë„êµ¬ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì €ì¥, ê´€ë¦¬, ë¶„ì„í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ë„˜ì–´ì„œëŠ” ëŒ€ëŸ‰ì˜ ì •í˜• ë˜ëŠ” ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë§í•œë‹¤."
    ]
    
    # SimpleRAG ì´ˆê¸°í™” (ë¡œì»¬ EXAONE ëª¨ë¸ ì‚¬ìš©)
    try:
        rag = SimpleRAG("local")
    except Exception as e:
        print(f"ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("OpenAI APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        rag = SimpleRAG("openai")
    
    # ë¬¸ì„œ ì¶”ê°€
    rag.add_documents(korean_documents)
    
    # ì§ˆë¬¸ë“¤
    questions = [
        "ì¸ê³µì§€ëŠ¥ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?",
        "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ë¹…ë°ì´í„°ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹¤í–‰
    for question in questions:
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("-" * 30)
        
        result = rag.query(question)
        
        print(f"ğŸ’¡ ë‹µë³€: {result['answer']}")
        similarities = [source['similarity_score'] for source in result['sources']]
        print(f"ğŸ“Š ìœ ì‚¬ë„: {[f'{sim:.3f}' for sim in similarities]}")
        print(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {len(result['sources'])}ê°œ")


def demo_english_rag():
    """ì˜ì–´ RAG ë°ëª¨"""
    print("\n\nğŸš€ English RAG System Demo")
    print("=" * 50)
    
    # ì˜ì–´ ë¬¸ì„œ ìƒ˜í”Œ
    english_documents = [
        "Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.",
        "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data.",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.",
        "Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world.",
        "Reinforcement learning is an area of machine learning where an agent learns to behave in an environment by performing actions and receiving rewards or penalties.",
        "Big data refers to extremely large datasets that may be analyzed computationally to reveal patterns, trends, and associations."
    ]
    
    # SimpleRAG ì´ˆê¸°í™”
    try:
        rag = SimpleRAG("local")
    except:
        rag = SimpleRAG("openai")
    
    # ë¬¸ì„œ ì¶”ê°€
    rag.add_documents(english_documents)
    
    # ì˜ì–´ ì§ˆë¬¸ë“¤
    questions = [
        "What is machine learning?",
        "How does deep learning differ from traditional machine learning?",
        "What is natural language processing used for?",
        "Explain computer vision"
    ]
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹¤í–‰
    for question in questions:
        print(f"\nâ“ Question: {question}")
        print("-" * 30)
        
        result = rag.query(question)
        
        print(f"ğŸ’¡ Answer: {result['answer']}")
        similarities = [source['similarity_score'] for source in result['sources']]
        print(f"ğŸ“Š Similarities: {[f'{sim:.3f}' for sim in similarities]}")
        print(f"ğŸ“š Sources: {len(result['sources'])} documents")


def interactive_rag():
    """ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ"""
    print("\n\nğŸ’¬ ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ê¸°ë³¸ ë¬¸ì„œ ë¡œë“œ
    documents = [
        "Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "DjangoëŠ” Pythonìœ¼ë¡œ ì‘ì„±ëœ ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ, ë¹ ë¥¸ ê°œë°œê³¼ ê¹”ë”í•œ ë””ìì¸ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "FastAPIëŠ” í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ ì›¹ APIë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ Python í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        "NumPyëŠ” Pythonì—ì„œ ê³¼í•™ ê³„ì‚°ì„ ìœ„í•œ ê¸°ë³¸ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤.",
        "PandasëŠ” ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
        "TensorFlowëŠ” Googleì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        "PyTorchëŠ” Facebookì—ì„œ ê°œë°œí•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
    ]
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        rag = SimpleRAG("local")
        print("âœ… ë¡œì»¬ EXAONE ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    except:
        try:
            rag = SimpleRAG("openai")
            print("âœ… OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except:
            rag = SimpleRAG("claude")
            print("âœ… Claude ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    rag.add_documents(documents)
    print(f"ğŸ“š {len(documents)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    while True:
        try:
            question = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë']:
                print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not question:
                continue
            
            print("ğŸ” ê²€ìƒ‰ ì¤‘...")
            result = rag.query(question)
            
            print(f"\nğŸ’¡ ë‹µë³€: {result['answer']}")
            print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(result['sources'])}")
            
            # ìƒì„¸ ì •ë³´ í‘œì‹œ ì—¬ë¶€ ë¬»ê¸°
            show_details = input("\nìƒì„¸ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if show_details in ['y', 'yes', 'ã…‡']:
                print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                for i, source in enumerate(result['sources'], 1):
                    print(f"  {i}. {source}")
                similarities = [source['similarity_score'] for source in result['sources']]
                print(f"\nğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {[f'{sim:.3f}' for sim in similarities]}")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ Hello RAG - ì²« ë²ˆì§¸ RAG ì˜ˆì œ")
    print("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
    available_providers = config_manager.get_available_providers()
    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì: {available_providers}")
    
    if not available_providers:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ Ollamaë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        # ë°ëª¨ ì‹¤í–‰
        demo_korean_rag()
        # demo_english_rag()
        
        # ëŒ€í™”í˜• RAG
        # interactive_rag()
        
    except Exception as e:
        logger.error(f"Demo ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("\nâŒ ë°ëª¨ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ì„¤ì •ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()