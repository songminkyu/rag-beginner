"""
Local Model Provider (EXAONE via Hugging Face Transformers)
EXAONE 4.0 ëª¨ë¸ì„ Hugging Face Transformersë¥¼ í†µí•´ ì‚¬ìš©í•˜ëŠ” ì œê³µì
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, List, Optional, Iterator
from sentence_transformers import SentenceTransformer
import logging
import gc
import ollama
from .base_provider import BaseLLMProvider, EmbeddingProvider, LLMResponse, ChatMessage, parse_llm_response

logger = logging.getLogger(__name__)


class LocalProvider(BaseLLMProvider):
    """ë¡œì»¬ LLM ì œê³µì (EXAONE via Hugging Face Transformers)"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model_name = config.get("model", "LGAI-EXAONE/EXAONE-4.0-1.2B")
        self.device = config.get("device", "auto")
        self.torch_dtype = config.get("torch_dtype", "bfloat16")
        self.korean_optimized = config.get("korean_optimized", True)
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        self.low_cpu_mem_usage = config.get("low_cpu_mem_usage", True)
        self.use_cache = config.get("use_cache", True)
        
        self.model = None
        self.tokenizer = None
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
    
    def _load_model(self):
        """EXAONE ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"Loading EXAONE model: {self.model_name}")
            
            # íŠ¹ë³„í•œ transformers ë²„ì „ í•„ìš” (EXAONE 4.0 ì§€ì›)
            try:
                # EXAONE 4.0 ì „ìš© transformers ì„¤ì¹˜ ì•ˆë‚´
                import transformers
                logger.info(f"Transformers version: {transformers.__version__}")
            except ImportError:
                logger.error("Please install transformers with EXAONE support:")
                logger.error("pip install git+https://github.com/lgai-exaone/transformers@add-exaone4")
                raise
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            logger.info("Tokenizer loaded successfully")
            
            # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
            torch_dtype = getattr(torch, self.torch_dtype) if isinstance(self.torch_dtype, str) else self.torch_dtype
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch_dtype,
                device_map=self.device,
                low_cpu_mem_usage=self.low_cpu_mem_usage,
                trust_remote_code=True
            )
            
            logger.info(f"Model loaded successfully on device: {self.model.device}")
            logger.info(f"Model dtype: {self.model.dtype}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                logger.info(f"GPU memory used: {memory_used:.2f} GB")
                
        except Exception as e:
            logger.error(f"Failed to load EXAONE model: {e}")
            raise
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> LLMResponse:
        """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        try:
            # ì±„íŒ… ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜
            messages = self._prepare_messages(prompt, system_prompt)
            
            # í† í¬ë‚˜ì´ì§•
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
            generation_kwargs = {
                "max_new_tokens": kwargs.get("max_tokens", self.max_tokens),
                "temperature": kwargs.get("temperature", self.temperature),
                "do_sample": kwargs.get("temperature", self.temperature) > 0,
                "top_p": kwargs.get("top_p", 0.95),
                "pad_token_id": self.tokenizer.eos_token_id,
                "use_cache": self.use_cache,
            }
            
            # EXAONE ëª¨ë¸ íŠ¹í™” ì„¤ì •
            if "exaone" in self.model_name.lower():
                generation_kwargs.update({
                    "repetition_penalty": kwargs.get("repetition_penalty", 1.0),  # EXAONEì—ì„œ ì¤‘ìš”
                    "length_penalty": kwargs.get("length_penalty", 1.0),
                })
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                output = self.model.generate(
                    input_ids.to(self.model.device),
                    **generation_kwargs
                )
            
            # ì…ë ¥ í† í° ì œê±°í•˜ê³  ì¶œë ¥ë§Œ ë””ì½”ë”©
            generated_tokens = output[0][len(input_ids[0]):]
            response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            return LLMResponse(
                content=response_text.strip(),
                model=self.model_name,
                metadata={
                    "provider": "local_transformers",
                    "input_length": len(input_ids[0]),
                    "output_length": len(generated_tokens),
                    "total_tokens": len(output[0]),
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            raise
    
    def chat(
        self,
        messages: List[ChatMessage],
        **kwargs
    ) -> LLMResponse:
        """ì±„íŒ… í˜•íƒœë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        try:
            # ChatMessageë¥¼ Hugging Face í˜•íƒœë¡œ ë³€í™˜
            hf_messages = []
            for msg in messages:
                hf_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # í† í¬ë‚˜ì´ì§•
            input_ids = self.tokenizer.apply_chat_template(
                hf_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
            generation_kwargs = {
                "max_new_tokens": kwargs.get("max_tokens", self.max_tokens),
                "temperature": kwargs.get("temperature", self.temperature),
                "do_sample": kwargs.get("temperature", self.temperature) > 0,
                "top_p": kwargs.get("top_p", 0.95),
                "pad_token_id": self.tokenizer.eos_token_id,
                "use_cache": self.use_cache,
            }
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                output = self.model.generate(
                    input_ids.to(self.model.device),
                    **generation_kwargs
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            generated_tokens = output[0][len(input_ids[0]):]
            response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            return LLMResponse(
                content=response_text.strip(),
                model=self.model_name,
                metadata={
                    "provider": "local_transformers",
                    "input_length": len(input_ids[0]),
                    "output_length": len(generated_tokens),
                }
            )
            
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            raise
    
    def stream_generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        # Hugging Face Transformersì˜ ìŠ¤íŠ¸ë¦¬ë°ì€ ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ ìƒì„± í›„ ì²­í¬ë¡œ ë°˜í™˜
        try:
            response = self.generate(prompt, system_prompt, **kwargs)
            
            # ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            words = response.content.split()
            for i, word in enumerate(words):
                if i == 0:
                    yield word
                else:
                    yield " " + word
                    
        except Exception as e:
            logger.error(f"Error in streaming generation: {e}")
            raise
    
    def stream_chat(
        self,
        messages: List[ChatMessage],
        **kwargs
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì±„íŒ…"""
        
        try:
            response = self.chat(messages, **kwargs)
            
            # ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            words = response.content.split()
            for i, word in enumerate(words):
                if i == 0:
                    yield word
                else:
                    yield " " + word
                    
        except Exception as e:
            logger.error(f"Error in streaming chat: {e}")
            raise
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì™¸ë¶€ ëª¨ë¸ ì‚¬ìš©)"""
        raise NotImplementedError("Use LocalEmbeddingProvider for embeddings")
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±"""
        raise NotImplementedError("Use LocalEmbeddingProvider for embeddings")
    
    def _prepare_messages(self, prompt: str, system_prompt: Optional[str] = None) -> List[Dict[str, str]]:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜"""
        
        messages = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        elif self.korean_optimized:
            # í•œêµ­ì–´ ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            messages.append({
                "role": "system", 
                "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
            })
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        messages.append({"role": "user", "content": prompt})
        
        return messages
    
    def validate_config(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            return self.model is not None and self.tokenizer is not None
        except:
            return False
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        logger.info("Model cleanup completed")


class LocalEmbeddingProvider(EmbeddingProvider):
    """ë¡œì»¬ ì„ë² ë”© ì œê³µì (SentenceTransformer ê¸°ë°˜)"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì„ íƒ
        if config.get("korean_optimized", True):
            # í•œêµ­ì–´-ì˜ì–´ ë‹¤êµ­ì–´ ëª¨ë¸
            self.model_name = config.get(
                "embedding_model",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
        else:
            self.model_name = config.get(
                "embedding_model",
                "sentence-transformers/all-MiniLM-L6-v2"
            )
        
        try:
            self.model = SentenceTransformer(self.model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            logger.info(f"Loaded embedding model: {self.model_name} (dim: {self.dimension})")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            raise
    
    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        try:
            embedding = self.model.encode(text, convert_to_tensor=False)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error embedding text: {e}")
            raise
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return [emb.tolist() for emb in embeddings]
        except Exception as e:
            logger.error(f"Error embedding texts: {e}")
            raise
    
    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.dimension


def create_local_provider(config: Dict[str, Any]) -> LocalProvider:
    """ë¡œì»¬ LLM ì œê³µì ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    return LocalProvider(config)


def create_local_embedding_provider(config: Dict[str, Any]) -> LocalEmbeddingProvider:
    """ë¡œì»¬ ì„ë² ë”© ì œê³µì ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    return LocalEmbeddingProvider(config)


def setup_exaone_model(model_name: str = "exaone-deep:32b", base_url: str = "http://localhost:11434"):
    """EXAONE ëª¨ë¸ ì„¤ì • ë° ë‹¤ìš´ë¡œë“œ"""
    
    try:
        client = ollama.Client(host=base_url)
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        try:
            client.list()
            logger.info("Ollama server is running")
        except:
            logger.error("Ollama server is not running. Please start Ollama first.")
            return False
        
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        models = client.list()
        model_names = [model['name'] for model in models['models']]
        
        if model_name not in model_names:
            logger.info(f"Downloading EXAONE model: {model_name}")
            logger.info("This may take a while for the first time...")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            client.pull(model_name)
            logger.info(f"Successfully downloaded {model_name}")
        else:
            logger.info(f"EXAONE model {model_name} is already available")
        
        # í…ŒìŠ¤íŠ¸ ìƒì„±
        logger.info("Testing model generation...")
        response = client.generate(
            model=model_name,
            prompt="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”.",
            options={'num_predict': 50}
        )
        
        logger.info(f"Test response: {response['response'][:100]}...")
        logger.info("EXAONE model setup completed successfully!")
        
        return True
        
    except Exception as e:
        logger.error(f"Error setting up EXAONE model: {e}")
        return False


def get_exaone_usage_guide() -> str:
    """EXAONE ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ"""
    
    guide = """
    ğŸš€ EXAONE 4.0 ì‚¬ìš© ê°€ì´ë“œ
    
    1. ëª¨ë¸ ì¢…ë¥˜:
       - exaone-deep:2.4b  (ì‘ì€ ëª¨ë¸, ë¹ ë¥¸ ì¶”ë¡ )
       - exaone-deep:7.8b  (ì¤‘ê°„ ëª¨ë¸, ê· í˜•ì¡íŒ ì„±ëŠ¥)
       - exaone-deep:32b   (í° ëª¨ë¸, ìµœê³  ì„±ëŠ¥)
    
    2. í•œêµ­ì–´ ìµœì í™”:
       - í•œêµ­ì–´ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€
       - í•œêµ­ ë¬¸í™”ì™€ ë§¥ë½ì„ ì´í•´
       - í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ì— ìµœì í™”
    
    3. ì¶”ë¡  ëª¨ë“œ:
       - EXAONE Deep ëª¨ë¸ì€ <thought> íƒœê·¸ë¥¼ ì‚¬ìš©
       - ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì¤Œ
       - ìˆ˜í•™ ë¬¸ì œë‚˜ ë³µì¡í•œ ì§ˆë¬¸ì— ìœ ìš©
    
    4. ì„±ëŠ¥ íŒ:
       - repeat_penaltyë¥¼ 1.0ìœ¼ë¡œ ì„¤ì • ê¶Œì¥
       - temperatureëŠ” 0.1-0.6 ì‚¬ì´ ê¶Œì¥
       - í•œêµ­ì–´ ì§ˆë¬¸ ì‹œ ë” ë‚˜ì€ ì„±ëŠ¥
    
    5. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­:
       - 2.4B: 4GB RAM
       - 7.8B: 8GB RAM  
       - 32B: 32GB RAM
    """
    
    return guide


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    from src.config.api_config import LocalModelConfig
    
    config = LocalModelConfig.from_env()
    
    # EXAONE ëª¨ë¸ ì„¤ì •
    setup_success = setup_exaone_model(config.model, config.base_url)
    
    if setup_success:
        # ë¡œì»¬ ì œê³µì í…ŒìŠ¤íŠ¸
        provider = LocalProvider(config.__dict__)
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        response = provider.generate("ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.")
        print(f"Response: {response.content}")
        
        # ì„ë² ë”© ì œê³µì í…ŒìŠ¤íŠ¸  
        embedding_provider = LocalEmbeddingProvider(config.__dict__)
        
        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        embedding = embedding_provider.embed_text("ì•ˆë…•í•˜ì„¸ìš”")
        print(f"Embedding dimension: {len(embedding)}")
        
        print(get_exaone_usage_guide())
    else:
        print("Failed to setup EXAONE model. Please check Ollama installation.")