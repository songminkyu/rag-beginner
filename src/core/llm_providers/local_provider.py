"""
Local Model Provider (EXAONE via Ollama)
EXAONE 4.0 ëª¨ë¸ì„ Ollamaë¥¼ í†µí•´ ì‚¬ìš©í•˜ëŠ” ì œê³µì
"""

import ollama
import requests
from typing import Dict, Any, List, Optional, Iterator
from sentence_transformers import SentenceTransformer
import logging

from .base_provider import BaseLLMProvider, EmbeddingProvider, LLMResponse, ChatMessage, parse_llm_response

logger = logging.getLogger(__name__)


class LocalLLMProvider(BaseLLMProvider):
    """ë¡œì»¬ LLM ì œê³µì (EXAONE via Ollama)"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://localhost:11434")
        self.client = ollama.Client(host=self.base_url)
        self.korean_optimized = config.get("korean_optimized", True)
        
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        self._ensure_model_available()
    
    def _ensure_model_available(self):
        """ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
            models = self.client.list()
            model_names = [model['name'] for model in models['models']]
            
            if self.model_name not in model_names:
                logger.info(f"Downloading model: {self.model_name}")
                self.client.pull(self.model_name)
                logger.info(f"Model {self.model_name} downloaded successfully")
            else:
                logger.info(f"Model {self.model_name} is already available")
                
        except Exception as e:
            logger.warning(f"Could not verify model availability: {e}")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> LLMResponse:
        """ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        try:
            # EXAONE ëª¨ë¸ì˜ íŠ¹ë³„í•œ í¬ë§· ì ìš©
            if "exaone" in self.model_name.lower():
                prompt = self._format_exaone_prompt(prompt, system_prompt)
            
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': kwargs.get('temperature', self.temperature),
                    'num_predict': kwargs.get('max_tokens', self.max_tokens),
                    'top_p': kwargs.get('top_p', 0.95),
                    'repeat_penalty': kwargs.get('repeat_penalty', 1.0),  # EXAONEì—ì„œ ì¤‘ìš”
                }
            )
            
            return LLMResponse(
                content=response['response'],
                model=self.model_name,
                metadata={
                    'provider': 'local',
                    'done': response.get('done', True),
                    'total_duration': response.get('total_duration'),
                    'load_duration': response.get('load_duration'),
                    'prompt_eval_count': response.get('prompt_eval_count'),
                    'eval_count': response.get('eval_count'),
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            raise
    
    def chat(
        self,
        messages: List[ChatMessage],
        **kwargs
    ) -> LLMResponse:
        """ì±„íŒ… í˜•íƒœë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        try:
            # ChatMessageë¥¼ Ollama í˜•íƒœë¡œ ë³€í™˜
            ollama_messages = []
            for msg in messages:
                ollama_messages.append({
                    'role': msg.role,
                    'content': msg.content
                })
            
            response = self.client.chat(
                model=self.model_name,
                messages=ollama_messages,
                options={
                    'temperature': kwargs.get('temperature', self.temperature),
                    'num_predict': kwargs.get('max_tokens', self.max_tokens),
                    'top_p': kwargs.get('top_p', 0.95),
                    'repeat_penalty': kwargs.get('repeat_penalty', 1.0),
                }
            )
            
            return LLMResponse(
                content=response['message']['content'],
                model=self.model_name,
                metadata={
                    'provider': 'local',
                    'done': response.get('done', True),
                    'total_duration': response.get('total_duration'),
                    'load_duration': response.get('load_duration'),
                    'prompt_eval_count': response.get('prompt_eval_count'),
                    'eval_count': response.get('eval_count'),
                }
            )
            
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            raise
    
    def stream_generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        try:
            if "exaone" in self.model_name.lower():
                prompt = self._format_exaone_prompt(prompt, system_prompt)
            
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                stream=True,
                options={
                    'temperature': kwargs.get('temperature', self.temperature),
                    'num_predict': kwargs.get('max_tokens', self.max_tokens),
                    'top_p': kwargs.get('top_p', 0.95),
                    'repeat_penalty': kwargs.get('repeat_penalty', 1.0),
                }
            )
            
            for chunk in response:
                if 'response' in chunk:
                    yield chunk['response']
                    
        except Exception as e:
            logger.error(f"Error in streaming generation: {e}")
            raise
    
    def stream_chat(
        self,
        messages: List[ChatMessage],
        **kwargs
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì±„íŒ…"""
        
        try:
            ollama_messages = []
            for msg in messages:
                ollama_messages.append({
                    'role': msg.role,
                    'content': msg.content
                })
            
            response = self.client.chat(
                model=self.model_name,
                messages=ollama_messages,
                stream=True,
                options={
                    'temperature': kwargs.get('temperature', self.temperature),
                    'num_predict': kwargs.get('max_tokens', self.max_tokens),
                    'top_p': kwargs.get('top_p', 0.95),
                    'repeat_penalty': kwargs.get('repeat_penalty', 1.0),
                }
            )
            
            for chunk in response:
                if 'message' in chunk and 'content' in chunk['message']:
                    yield chunk['message']['content']
                    
        except Exception as e:
            logger.error(f"Error in streaming chat: {e}")
            raise
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì™¸ë¶€ ëª¨ë¸ ì‚¬ìš©)"""
        # OllamaëŠ” í˜„ì¬ ì„ë² ë”©ì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 
        # SentenceTransformer ë“±ì„ ì‚¬ìš©
        raise NotImplementedError("Use LocalEmbeddingProvider for embeddings")
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±"""
        raise NotImplementedError("Use LocalEmbeddingProvider for embeddings")
    
    def _format_exaone_prompt(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """EXAONE ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        
        # EXAONE Deep ëª¨ë¸ì€ <thought> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ê³¼ì •ì„ í‘œì‹œ
        if "deep" in self.model_name.lower():
            if system_prompt:
                formatted = f"{system_prompt}\n\n<thought>\nì´ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.\n</thought>\n\n{prompt}"
            else:
                formatted = f"<thought>\n{prompt}ì— ëŒ€í•´ ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤.\n</thought>\n\n{prompt}"
        else:
            # ì¼ë°˜ EXAONE ëª¨ë¸
            if system_prompt:
                formatted = f"{system_prompt}\n\n{prompt}"
            else:
                formatted = prompt
        
        return formatted
    
    def validate_config(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # Ollama ì„œë²„ ì—°ê²° í™•ì¸
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        try:
            models = self.client.list()
            return [model['name'] for model in models['models']]
        except:
            return []


class LocalEmbeddingProvider(EmbeddingProvider):
    """ë¡œì»¬ ì„ë² ë”© ì œê³µì (SentenceTransformer ê¸°ë°˜)"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì„ íƒ
        if config.get("korean_optimized", True):
            # í•œêµ­ì–´-ì˜ì–´ ë‹¤êµ­ì–´ ëª¨ë¸
            self.model_name = config.get(
                "embedding_model",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
        else:
            self.model_name = config.get(
                "embedding_model",
                "sentence-transformers/all-MiniLM-L6-v2"
            )
        
        try:
            self.model = SentenceTransformer(self.model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            logger.info(f"Loaded embedding model: {self.model_name} (dim: {self.dimension})")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            raise
    
    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        try:
            embedding = self.model.encode(text, convert_to_tensor=False)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error embedding text: {e}")
            raise
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return [emb.tolist() for emb in embeddings]
        except Exception as e:
            logger.error(f"Error embedding texts: {e}")
            raise
    
    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.dimension


def create_local_provider(config: Dict[str, Any]) -> LocalLLMProvider:
    """ë¡œì»¬ LLM ì œê³µì ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    return LocalLLMProvider(config)


def create_local_embedding_provider(config: Dict[str, Any]) -> LocalEmbeddingProvider:
    """ë¡œì»¬ ì„ë² ë”© ì œê³µì ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    return LocalEmbeddingProvider(config)


def setup_exaone_model(model_name: str = "exaone-deep:32b", base_url: str = "http://localhost:11434"):
    """EXAONE ëª¨ë¸ ì„¤ì • ë° ë‹¤ìš´ë¡œë“œ"""
    
    try:
        client = ollama.Client(host=base_url)
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        try:
            client.list()
            logger.info("Ollama server is running")
        except:
            logger.error("Ollama server is not running. Please start Ollama first.")
            return False
        
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        models = client.list()
        model_names = [model['name'] for model in models['models']]
        
        if model_name not in model_names:
            logger.info(f"Downloading EXAONE model: {model_name}")
            logger.info("This may take a while for the first time...")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            client.pull(model_name)
            logger.info(f"Successfully downloaded {model_name}")
        else:
            logger.info(f"EXAONE model {model_name} is already available")
        
        # í…ŒìŠ¤íŠ¸ ìƒì„±
        logger.info("Testing model generation...")
        response = client.generate(
            model=model_name,
            prompt="ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”.",
            options={'num_predict': 50}
        )
        
        logger.info(f"Test response: {response['response'][:100]}...")
        logger.info("EXAONE model setup completed successfully!")
        
        return True
        
    except Exception as e:
        logger.error(f"Error setting up EXAONE model: {e}")
        return False


def get_exaone_usage_guide() -> str:
    """EXAONE ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ"""
    
    guide = """
    ğŸš€ EXAONE 4.0 ì‚¬ìš© ê°€ì´ë“œ
    
    1. ëª¨ë¸ ì¢…ë¥˜:
       - exaone-deep:2.4b  (ì‘ì€ ëª¨ë¸, ë¹ ë¥¸ ì¶”ë¡ )
       - exaone-deep:7.8b  (ì¤‘ê°„ ëª¨ë¸, ê· í˜•ì¡íŒ ì„±ëŠ¥)
       - exaone-deep:32b   (í° ëª¨ë¸, ìµœê³  ì„±ëŠ¥)
    
    2. í•œêµ­ì–´ ìµœì í™”:
       - í•œêµ­ì–´ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€
       - í•œêµ­ ë¬¸í™”ì™€ ë§¥ë½ì„ ì´í•´
       - í•œêµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ì— ìµœì í™”
    
    3. ì¶”ë¡  ëª¨ë“œ:
       - EXAONE Deep ëª¨ë¸ì€ <thought> íƒœê·¸ë¥¼ ì‚¬ìš©
       - ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì¤Œ
       - ìˆ˜í•™ ë¬¸ì œë‚˜ ë³µì¡í•œ ì§ˆë¬¸ì— ìœ ìš©
    
    4. ì„±ëŠ¥ íŒ:
       - repeat_penaltyë¥¼ 1.0ìœ¼ë¡œ ì„¤ì • ê¶Œì¥
       - temperatureëŠ” 0.1-0.6 ì‚¬ì´ ê¶Œì¥
       - í•œêµ­ì–´ ì§ˆë¬¸ ì‹œ ë” ë‚˜ì€ ì„±ëŠ¥
    
    5. ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­:
       - 2.4B: 4GB RAM
       - 7.8B: 8GB RAM  
       - 32B: 32GB RAM
    """
    
    return guide


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    from config.api_config import LocalModelConfig
    
    config = LocalModelConfig.from_env()
    
    # EXAONE ëª¨ë¸ ì„¤ì •
    setup_success = setup_exaone_model(config.model, config.base_url)
    
    if setup_success:
        # ë¡œì»¬ ì œê³µì í…ŒìŠ¤íŠ¸
        provider = LocalLLMProvider(config.__dict__)
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        response = provider.generate("ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.")
        print(f"Response: {response.content}")
        
        # ì„ë² ë”© ì œê³µì í…ŒìŠ¤íŠ¸  
        embedding_provider = LocalEmbeddingProvider(config.__dict__)
        
        # ì„ë² ë”© í…ŒìŠ¤íŠ¸
        embedding = embedding_provider.embed_text("ì•ˆë…•í•˜ì„¸ìš”")
        print(f"Embedding dimension: {len(embedding)}")
        
        print(get_exaone_usage_guide())
    else:
        print("Failed to setup EXAONE model. Please check Ollama installation.")