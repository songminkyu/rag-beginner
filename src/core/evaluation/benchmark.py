"""
Benchmark Module
ë‹¤ì–‘í•œ RAG ì‹œìŠ¤í…œì„ ë¹„êµ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
"""

import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from .rag_evaluator import RAGEvaluator, EvaluationDataset, EvaluationResult

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í´ë˜ìŠ¤"""
    benchmark_name: str
    systems: List[str]
    datasets: List[str]
    results: Dict[str, Dict[str, EvaluationResult]]  # system -> dataset -> result
    comparison_metrics: Dict[str, Dict[str, float]]  # metric -> system -> score
    rankings: Dict[str, List[str]]  # metric -> ranked_systems
    benchmark_time: float
    metadata: Optional[Dict[str, Any]] = None
    
    def get_best_system(self, metric: str) -> Optional[str]:
        """íŠ¹ì • ë©”íŠ¸ë¦­ì—ì„œ ìµœê³  ì„±ëŠ¥ ì‹œìŠ¤í…œ ë°˜í™˜"""
        
        if metric not in self.rankings:
            return None
        
        return self.rankings[metric][0] if self.rankings[metric] else None
    
    def get_system_ranking(self, system: str, metric: str) -> Optional[int]:
        """íŠ¹ì • ì‹œìŠ¤í…œì˜ ë©”íŠ¸ë¦­ë³„ ìˆœìœ„ ë°˜í™˜ (1ë¶€í„° ì‹œì‘)"""
        
        if metric not in self.rankings or system not in self.rankings[metric]:
            return None
        
        return self.rankings[metric].index(system) + 1
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        
        # EvaluationResultë¥¼ dictë¡œ ë³€í™˜
        results_dict = {}
        for system, dataset_results in self.results.items():
            results_dict[system] = {}
            for dataset, result in dataset_results.items():
                results_dict[system][dataset] = result.to_dict()
        
        return {
            "benchmark_name": self.benchmark_name,
            "systems": self.systems,
            "datasets": self.datasets,
            "results": results_dict,
            "comparison_metrics": self.comparison_metrics,
            "rankings": self.rankings,
            "benchmark_time": self.benchmark_time,
            "metadata": self.metadata
        }
    
    def save(self, filepath: Union[str, Path]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filepath}")


class RAGBenchmark:
    """RAG ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.benchmark_name = config.get("benchmark_name", "RAG_Benchmark")
        self.parallel_execution = config.get("parallel_execution", False)
        self.max_workers = config.get("max_workers", 2)
        
        # ë¹„êµí•  ë©”íŠ¸ë¦­ ëª©ë¡
        self.comparison_metrics = config.get("comparison_metrics", [
            "precision@5", "recall@5", "ndcg@5", "mrr",
            "bleu_1", "rouge1_f1", "semantic_similarity",
            "faithfulness", "answer_relevancy", "context_precision", "context_recall"
        ])
        
        logger.info(f"RAG ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”: {self.benchmark_name}")
    
    def run_benchmark(
        self,
        systems: Dict[str, RAGEvaluator],
        datasets: Dict[str, EvaluationDataset],
        save_individual_results: bool = True,
        results_dir: Optional[Union[str, Path]] = None
    ) -> BenchmarkResult:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        start_time = time.time()
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {len(systems)}ê°œ ì‹œìŠ¤í…œ, {len(datasets)}ê°œ ë°ì´í„°ì…‹")
        
        if results_dir:
            results_dir = Path(results_dir)
            results_dir.mkdir(parents=True, exist_ok=True)
        
        # í‰ê°€ ì‹¤í–‰
        all_results = {}
        
        if self.parallel_execution:
            all_results = self._run_parallel_evaluation(systems, datasets)
        else:
            all_results = self._run_sequential_evaluation(systems, datasets)
        
        # ê°œë³„ ê²°ê³¼ ì €ì¥
        if save_individual_results and results_dir:
            self._save_individual_results(all_results, results_dir)
        
        # ë¹„êµ ë©”íŠ¸ë¦­ ê³„ì‚°
        comparison_metrics = self._calculate_comparison_metrics(all_results)
        
        # ìˆœìœ„ ê³„ì‚°
        rankings = self._calculate_rankings(comparison_metrics)
        
        benchmark_time = time.time() - start_time
        
        benchmark_result = BenchmarkResult(
            benchmark_name=self.benchmark_name,
            systems=list(systems.keys()),
            datasets=list(datasets.keys()),
            results=all_results,
            comparison_metrics=comparison_metrics,
            rankings=rankings,
            benchmark_time=benchmark_time,
            metadata={
                "config": self.config,
                "comparison_metrics": self.comparison_metrics,
                "parallel_execution": self.parallel_execution
            }
        )
        
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {benchmark_time:.2f}ì´ˆ")
        return benchmark_result
    
    def _run_sequential_evaluation(
        self,
        systems: Dict[str, RAGEvaluator],
        datasets: Dict[str, EvaluationDataset]
    ) -> Dict[str, Dict[str, EvaluationResult]]:
        """ìˆœì°¨ í‰ê°€ ì‹¤í–‰"""
        
        all_results = {}
        total_evaluations = len(systems) * len(datasets)
        current_eval = 0
        
        for system_name, evaluator in systems.items():
            all_results[system_name] = {}
            
            for dataset_name, dataset in datasets.items():
                current_eval += 1
                logger.info(f"í‰ê°€ ì§„í–‰ ({current_eval}/{total_evaluations}): {system_name} - {dataset_name}")
                
                try:
                    result = evaluator.evaluate(dataset, dataset_name)
                    all_results[system_name][dataset_name] = result
                    
                except Exception as e:
                    logger.error(f"í‰ê°€ ì‹¤íŒ¨ ({system_name} - {dataset_name}): {e}")
                    # ë¹ˆ ê²°ê³¼ë¡œ ëŒ€ì²´
                    empty_result = EvaluationResult(
                        dataset_name=dataset_name,
                        total_questions=len(dataset),
                        evaluation_time=0.0,
                        retrieval_metrics={},
                        generation_metrics={},
                        rag_metrics={},
                        per_question_results=[],
                        metadata={"error": str(e)}
                    )
                    all_results[system_name][dataset_name] = empty_result
        
        return all_results
    
    def _run_parallel_evaluation(
        self,
        systems: Dict[str, RAGEvaluator],
        datasets: Dict[str, EvaluationDataset]
    ) -> Dict[str, Dict[str, EvaluationResult]]:
        """ë³‘ë ¬ í‰ê°€ ì‹¤í–‰"""
        
        all_results = {system_name: {} for system_name in systems.keys()}
        
        # í‰ê°€ ì‘ì—… ë¦¬ìŠ¤íŠ¸ ìƒì„±
        evaluation_tasks = []
        for system_name, evaluator in systems.items():
            for dataset_name, dataset in datasets.items():
                evaluation_tasks.append((system_name, evaluator, dataset_name, dataset))
        
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_task = {}
            for task in evaluation_tasks:
                future = executor.submit(self._evaluate_single, *task)
                future_to_task[future] = task
            
            # ê²°ê³¼ ìˆ˜ì§‘
            completed = 0
            for future in as_completed(future_to_task):
                system_name, _, dataset_name, _ = future_to_task[future]
                completed += 1
                
                try:
                    result = future.result()
                    all_results[system_name][dataset_name] = result
                    logger.info(f"í‰ê°€ ì™„ë£Œ ({completed}/{len(evaluation_tasks)}): {system_name} - {dataset_name}")
                    
                except Exception as e:
                    logger.error(f"í‰ê°€ ì‹¤íŒ¨ ({system_name} - {dataset_name}): {e}")
                    # ë¹ˆ ê²°ê³¼ë¡œ ëŒ€ì²´
                    empty_result = EvaluationResult(
                        dataset_name=dataset_name,
                        total_questions=0,
                        evaluation_time=0.0,
                        retrieval_metrics={},
                        generation_metrics={},
                        rag_metrics={},
                        per_question_results=[],
                        metadata={"error": str(e)}
                    )
                    all_results[system_name][dataset_name] = empty_result
        
        return all_results
    
    def _evaluate_single(
        self,
        system_name: str,
        evaluator: RAGEvaluator,
        dataset_name: str,
        dataset: EvaluationDataset
    ) -> EvaluationResult:
        """ë‹¨ì¼ í‰ê°€ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        
        return evaluator.evaluate(dataset, dataset_name)
    
    def _save_individual_results(
        self,
        all_results: Dict[str, Dict[str, EvaluationResult]],
        results_dir: Path
    ):
        """ê°œë³„ ê²°ê³¼ ì €ì¥"""
        
        for system_name, dataset_results in all_results.items():
            system_dir = results_dir / system_name
            system_dir.mkdir(exist_ok=True)
            
            for dataset_name, result in dataset_results.items():
                result_file = system_dir / f"{dataset_name}_result.json"
                result.save(result_file)
    
    def _calculate_comparison_metrics(
        self,
        all_results: Dict[str, Dict[str, EvaluationResult]]
    ) -> Dict[str, Dict[str, float]]:
        """ë¹„êµ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        comparison_metrics = {}
        
        for metric_name in self.comparison_metrics:
            comparison_metrics[metric_name] = {}
            
            for system_name, dataset_results in all_results.items():
                scores = []
                
                for dataset_name, result in dataset_results.items():
                    # ë©”íŠ¸ë¦­ ê°’ ì¶”ì¶œ
                    score = self._extract_metric_value(result, metric_name)
                    if score is not None:
                        scores.append(score)
                
                # í‰ê·  ì ìˆ˜ ê³„ì‚°
                if scores:
                    avg_score = sum(scores) / len(scores)
                    comparison_metrics[metric_name][system_name] = avg_score
                else:
                    comparison_metrics[metric_name][system_name] = 0.0
        
        return comparison_metrics
    
    def _extract_metric_value(
        self,
        result: EvaluationResult,
        metric_name: str
    ) -> Optional[float]:
        """í‰ê°€ ê²°ê³¼ì—ì„œ íŠ¹ì • ë©”íŠ¸ë¦­ ê°’ ì¶”ì¶œ"""
        
        # ê²€ìƒ‰ ë©”íŠ¸ë¦­ì—ì„œ ì°¾ê¸°
        if metric_name in result.retrieval_metrics:
            return result.retrieval_metrics[metric_name]
        
        # ìƒì„± ë©”íŠ¸ë¦­ì—ì„œ ì°¾ê¸°
        if metric_name in result.generation_metrics:
            return result.generation_metrics[metric_name]
        
        # RAG ë©”íŠ¸ë¦­ì—ì„œ ì°¾ê¸°
        if metric_name in result.rag_metrics:
            return result.rag_metrics[metric_name]
        
        return None
    
    def _calculate_rankings(
        self,
        comparison_metrics: Dict[str, Dict[str, float]]
    ) -> Dict[str, List[str]]:
        """ì‹œìŠ¤í…œ ìˆœìœ„ ê³„ì‚°"""
        
        rankings = {}
        
        for metric_name, system_scores in comparison_metrics.items():
            # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
            sorted_systems = sorted(
                system_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            rankings[metric_name] = [system_name for system_name, _ in sorted_systems]
        
        return rankings
    
    def generate_report(
        self,
        benchmark_result: BenchmarkResult,
        output_file: Optional[Union[str, Path]] = None
    ) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report_lines = []
        
        # í—¤ë”
        report_lines.append(f"# {benchmark_result.benchmark_name} ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸")
        report_lines.append("")
        report_lines.append(f"**ë²¤ì¹˜ë§ˆí¬ ì‹œê°„**: {benchmark_result.benchmark_time:.2f}ì´ˆ")
        report_lines.append(f"**í‰ê°€ ì‹œìŠ¤í…œ**: {len(benchmark_result.systems)}ê°œ")
        report_lines.append(f"**í‰ê°€ ë°ì´í„°ì…‹**: {len(benchmark_result.datasets)}ê°œ")
        report_lines.append("")
        
        # ì „ì²´ ìˆœìœ„í‘œ
        report_lines.append("## ğŸ“Š ì¢…í•© ìˆœìœ„")
        report_lines.append("")
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ë³„ ìˆœìœ„
        main_metrics = ["faithfulness", "answer_relevancy", "precision@5", "rouge1_f1"]
        
        for metric in main_metrics:
            if metric in benchmark_result.rankings:
                report_lines.append(f"### {metric}")
                for i, system in enumerate(benchmark_result.rankings[metric], 1):
                    score = benchmark_result.comparison_metrics[metric][system]
                    report_lines.append(f"{i}. **{system}**: {score:.4f}")
                report_lines.append("")
        
        # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
        report_lines.append("## ğŸ“ˆ ìƒì„¸ ê²°ê³¼")
        report_lines.append("")
        
        # ë©”íŠ¸ë¦­ë³„ í…Œì´ë¸”
        for metric_name in self.comparison_metrics:
            if metric_name in benchmark_result.comparison_metrics:
                report_lines.append(f"### {metric_name}")
                report_lines.append("")
                
                # í…Œì´ë¸” í—¤ë”
                header = "| ì‹œìŠ¤í…œ | ì ìˆ˜ | ìˆœìœ„ |"
                separator = "|--------|------|------|"
                report_lines.append(header)
                report_lines.append(separator)
                
                # í…Œì´ë¸” ë‚´ìš©
                for i, system in enumerate(benchmark_result.rankings[metric_name], 1):
                    score = benchmark_result.comparison_metrics[metric_name][system]
                    report_lines.append(f"| {system} | {score:.4f} | {i} |")
                
                report_lines.append("")
        
        # ì‹œìŠ¤í…œë³„ ìƒì„¸ ë¶„ì„
        report_lines.append("## ğŸ” ì‹œìŠ¤í…œë³„ ë¶„ì„")
        report_lines.append("")
        
        for system_name in benchmark_result.systems:
            report_lines.append(f"### {system_name}")
            report_lines.append("")
            
            # ê°•ì ê³¼ ì•½ì  ë¶„ì„
            strengths = []
            weaknesses = []
            
            for metric_name in self.comparison_metrics:
                if metric_name in benchmark_result.rankings:
                    rank = benchmark_result.get_system_ranking(system_name, metric_name)
                    if rank and rank <= 2:  # ìƒìœ„ 2ìœ„ ì´ë‚´
                        strengths.append(f"{metric_name} ({rank}ìœ„)")
                    elif rank and rank >= len(benchmark_result.systems) - 1:  # í•˜ìœ„ 2ìœ„ ì´ë‚´
                        weaknesses.append(f"{metric_name} ({rank}ìœ„)")
            
            if strengths:
                report_lines.append(f"**ê°•ì **: {', '.join(strengths)}")
            if weaknesses:
                report_lines.append(f"**ì•½ì **: {', '.join(weaknesses)}")
            
            report_lines.append("")
        
        # ì „ì²´ ë¦¬í¬íŠ¸ ê²°í•©
        report = "\n".join(report_lines)
        
        # íŒŒì¼ ì €ì¥
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        return report


class BenchmarkSuite:
    """ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ - ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ ê´€ë¦¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.suite_name = config.get("suite_name", "RAG_Benchmark_Suite")
        self.benchmarks: List[RAGBenchmark] = []
        
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”: {self.suite_name}")
    
    def add_benchmark(self, benchmark: RAGBenchmark):
        """ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€"""
        self.benchmarks.append(benchmark)
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€: {benchmark.benchmark_name}")
    
    def run_all_benchmarks(
        self,
        systems: Dict[str, RAGEvaluator],
        datasets: Dict[str, EvaluationDataset],
        results_dir: Optional[Union[str, Path]] = None
    ) -> List[BenchmarkResult]:
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        if results_dir:
            results_dir = Path(results_dir)
            results_dir.mkdir(parents=True, exist_ok=True)
        
        all_results = []
        
        for i, benchmark in enumerate(self.benchmarks, 1):
            logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ({i}/{len(self.benchmarks)}): {benchmark.benchmark_name}")
            
            # ë²¤ì¹˜ë§ˆí¬ë³„ ê²°ê³¼ ë””ë ‰í† ë¦¬
            benchmark_results_dir = None
            if results_dir:
                benchmark_results_dir = results_dir / benchmark.benchmark_name
            
            try:
                result = benchmark.run_benchmark(
                    systems, 
                    datasets,
                    save_individual_results=True,
                    results_dir=benchmark_results_dir
                )
                all_results.append(result)
                
                # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
                if results_dir:
                    result_file = results_dir / f"{benchmark.benchmark_name}_benchmark.json"
                    result.save(result_file)
                
            except Exception as e:
                logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨ ({benchmark.benchmark_name}): {e}")
        
        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì™„ë£Œ: {len(all_results)}ê°œ ë²¤ì¹˜ë§ˆí¬")
        return all_results


def create_benchmark_suite(
    benchmark_configs: List[Dict[str, Any]],
    suite_config: Optional[Dict[str, Any]] = None
) -> BenchmarkSuite:
    """ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    
    suite_config = suite_config or {}
    suite = BenchmarkSuite(suite_config)
    
    for config in benchmark_configs:
        benchmark = RAGBenchmark(config)
        suite.add_benchmark(benchmark)
    
    return suite


def create_standard_benchmark_suite() -> BenchmarkSuite:
    """í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ìƒì„±"""
    
    benchmark_configs = [
        {
            "benchmark_name": "retrieval_benchmark",
            "comparison_metrics": [
                "precision@1", "precision@3", "precision@5",
                "recall@1", "recall@3", "recall@5",
                "ndcg@5", "mrr"
            ]
        },
        {
            "benchmark_name": "generation_benchmark", 
            "comparison_metrics": [
                "bleu_1", "bleu_2", "bleu_4",
                "rouge1_f1", "rouge2_f1", "rougeL_f1",
                "semantic_similarity"
            ]
        },
        {
            "benchmark_name": "end_to_end_benchmark",
            "comparison_metrics": [
                "faithfulness", "answer_relevancy",
                "context_precision", "context_recall"
            ]
        }
    ]
    
    suite_config = {
        "suite_name": "Standard_RAG_Benchmark_Suite"
    }
    
    return create_benchmark_suite(benchmark_configs, suite_config)